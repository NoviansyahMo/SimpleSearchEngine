{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimpleSearchEngine.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltj6WhApY1a1"
      },
      "source": [
        "import pandas as pd\r\n",
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk import FreqDist\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "def load_data(path):  \r\n",
        "  dataframe = pd.read_csv(path)  \r\n",
        "  return dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6oMm89caM2y"
      },
      "source": [
        "document_1 = \"I love watching movies when it's cold outside\"\r\n",
        "document_2 = \"Toy Story is the best animation movie ever, I love it!\"\r\n",
        "document_3 = \"Watching horror movies alone at night is really scary\"\r\n",
        "document_4 = \"He loves to watch films filled with suspense and unexpected plot twists\"\r\n",
        "document_5 = \"My mom loves to watch movies. My dad hates movie theaters. My brothers like any kind of movie. And I haven't watched a single movie since I got into college\"\r\n",
        "documents = [document_1, document_2, document_3, document_4, document_5]\r\n",
        "documents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soZaHCnZaUUS"
      },
      "source": [
        "tokens = sum([word_tokenize(document) for document in documents], [])\r\n",
        "words_frequency = FreqDist(tokens)\r\n",
        "words_frequency.plot(30, cumulative = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzTn1sGghR-9"
      },
      "source": [
        "import math\r\n",
        "\r\n",
        "def normalized_term_frequency(word, document):\r\n",
        "    raw_frequency = document.count(word)\r\n",
        "    if raw_frequency == 0:\r\n",
        "        return 0\r\n",
        "    return 1 + math.log(raw_frequency)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbQhmBBWhXSj"
      },
      "source": [
        "def docs_contain_word(word, documents):\r\n",
        "\tcounter = 0\r\n",
        "\tfor document in list_of_documents:\r\n",
        "\t\tif word in document:\r\n",
        "\t\t\tcounter+=1\r\n",
        "\t\r\n",
        "\treturn counter\r\n",
        "\r\n",
        "def get_vocabulary(documents):\r\n",
        "\tvocabulary = set([word for document in documents for word in document])\t\r\n",
        "\t\r\n",
        "\treturn vocabulary\r\n",
        "\r\n",
        "def inverse_document_frequency(documents, vocabulary):\r\n",
        "\r\n",
        "\tidf = {}\r\n",
        "\t\r\n",
        "\tfor word in vocabulary:\r\n",
        "\t\tcontains_word = docs_contain_word(word, documents)\r\n",
        "\t\tidf[word] = 1 + math.log(len(documents)/(contains_word))\r\n",
        "        \r\n",
        "\treturn idf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmSmXltlheXm"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "def tf_idf(search_keys, dataframe):\r\n",
        "  \r\n",
        "\ttfidf_vectorizer = TfidfVectorizer()\r\n",
        "\ttfidf_weights_matrix = tfidf_vectorizer.fit_transform(dataframe)\r\n",
        "\tkeys_weights_matrix = tfidf_vectorizer.transform([search_keys])\r\n",
        "\t\r\n",
        "\treturn keys_weights_matrix, tfidf_weights_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_J5tSIeuU_f"
      },
      "source": [
        "key, matriks = tf_idf('I like movies', documents)\r\n",
        "print(matriks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAbhUf08hwBo"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "\r\n",
        "def cos_similarity(search_query_weights, tfidf_weights_matrix):\r\n",
        "\t\r\n",
        "\tcosine_distance = cosine_similarity(search_query_weights, tfidf_weights_matrix)\r\n",
        "\tsimilarity_list = cosine_distance[0]\r\n",
        "  \r\n",
        "\treturn similarity_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS2MaQqPuret"
      },
      "source": [
        "similarity_list = cos_similarity(key, matriks)\r\n",
        "print(similarity_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP-8A3wmhsw6"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "def most_similar(similarity_list, min_talks=1):\r\n",
        "\t\r\n",
        "\tmost_similar= []\r\n",
        "  \r\n",
        "\twhile min_talks > 0:\r\n",
        "\t\ttmp_index = np.argmax(similarity_list)\r\n",
        "\t\tmost_similar.append(tmp_index)\r\n",
        "\t\tsimilarity_list[tmp_index] = 0\r\n",
        "\t\tmin_talks -= 1\r\n",
        "\r\n",
        "\treturn most_similar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Uy9_yeWvJbo"
      },
      "source": [
        "most = most_similar(similarity_list, min_talks=1)\r\n",
        "print(documents[4])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}